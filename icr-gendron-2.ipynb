{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432cef56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T00:19:32.406279Z",
     "iopub.status.busy": "2023-07-02T00:19:32.405770Z",
     "iopub.status.idle": "2023-07-02T00:19:32.413314Z",
     "shell.execute_reply": "2023-07-02T00:19:32.411872Z"
    },
    "papermill": {
     "duration": 0.030411,
     "end_time": "2023-07-02T00:19:32.420388",
     "exception": false,
     "start_time": "2023-07-02T00:19:32.389977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT first with internet option turned on\n",
    "# Use GPU env\n",
    "\n",
    "# !pip download tabpfn --no-deps -d pip-packages\n",
    "\n",
    "# from tabpfn import TabPFNClassifier\n",
    "# TabPFNClassifier(N_ensemble_configurations = 64, device = 'cuda:0')\n",
    "\n",
    "# !mv /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/prior_diff_real_checkpoint_n_0_epoch_100.cpkt pip-packages/\n",
    "# !zip -r pip-packages.zip pip-packages\n",
    "\n",
    "# now you need to download the zip and upload it as dataset with the plus in the top left\n",
    "# then you need to add it to the notebook as data on the right, and name it `pip-packages-icr`\n",
    "\n",
    "# now you can turn internet off and still install, like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904adcba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T00:19:32.446213Z",
     "iopub.status.busy": "2023-07-02T00:19:32.445818Z",
     "iopub.status.idle": "2023-07-02T00:19:48.156330Z",
     "shell.execute_reply": "2023-07-02T00:19:48.154530Z"
    },
    "papermill": {
     "duration": 15.727067,
     "end_time": "2023-07-02T00:19:48.159297",
     "exception": false,
     "start_time": "2023-07-02T00:19:32.432230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr/pip-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754a6f17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T00:19:48.185123Z",
     "iopub.status.busy": "2023-07-02T00:19:48.184705Z",
     "iopub.status.idle": "2023-07-02T00:19:52.200658Z",
     "shell.execute_reply": "2023-07-02T00:19:52.198919Z"
    },
    "papermill": {
     "duration": 4.03287,
     "end_time": "2023-07-02T00:19:52.203865",
     "exception": false,
     "start_time": "2023-07-02T00:19:48.170995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "# !cp /kaggle/input/pip-packages-icr/pip-packages/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b6ed41f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T00:19:52.231068Z",
     "iopub.status.busy": "2023-07-02T00:19:52.229337Z",
     "iopub.status.idle": "2023-07-02T00:19:59.633341Z",
     "shell.execute_reply": "2023-07-02T00:19:59.632047Z"
    },
    "papermill": {
     "duration": 7.42178,
     "end_time": "2023-07-02T00:19:59.637027",
     "exception": false,
     "start_time": "2023-07-02T00:19:52.215247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyprojroot import here\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a9ac510",
   "metadata": {
    "papermill": {
     "duration": 0.01148,
     "end_time": "2023-07-02T00:19:59.660545",
     "exception": false,
     "start_time": "2023-07-02T00:19:59.649065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad76a758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T00:19:59.686195Z",
     "iopub.status.busy": "2023-07-02T00:19:59.685504Z",
     "iopub.status.idle": "2023-07-02T00:19:59.788122Z",
     "shell.execute_reply": "2023-07-02T00:19:59.786815Z"
    },
    "papermill": {
     "duration": 0.118784,
     "end_time": "2023-07-02T00:19:59.791151",
     "exception": false,
     "start_time": "2023-07-02T00:19:59.672367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "local_dir = str(here()) + '/'\n",
    "kaggle_dir = '/kaggle/input/'\n",
    "train_df = pd.read_csv(local_dir + 'icr-identify-age-related-conditions/train.csv')\n",
    "test_df = pd.read_csv(local_dir + 'icr-identify-age-related-conditions/test.csv')\n",
    "greeks_df = pd.read_csv(local_dir + 'icr-identify-age-related-conditions/greeks.csv')\n",
    "\n",
    "# join greeks and add Epsilon\n",
    "train_df = pd.merge(train_df, greeks_df, on = 'Id')\n",
    "train_df = train_df.drop(['Id', 'Beta', 'Gamma', 'Delta'], axis = 1)\n",
    "train_df['Epsilon'] = train_df['Epsilon'].replace('Unknown', np.nan)\n",
    "train_df = train_df[train_df['Epsilon'].notna()]\n",
    "train_df['Epsilon'] = pd.to_datetime(train_df['Epsilon'])\n",
    "\n",
    "# change epsilon to days since 1-1-2019 when data started to pick up\n",
    "train_df['Days Since 1-1-2019'] = (train_df['Epsilon'] - pd.to_datetime('2019-01-01')).dt.days\n",
    "train_df = train_df.drop('Epsilon', axis = 1)\n",
    "train_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd377d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T00:19:59.816083Z",
     "iopub.status.busy": "2023-07-02T00:19:59.815658Z",
     "iopub.status.idle": "2023-07-02T00:19:59.821960Z",
     "shell.execute_reply": "2023-07-02T00:19:59.820629Z"
    },
    "papermill": {
     "duration": 0.021891,
     "end_time": "2023-07-02T00:19:59.824688",
     "exception": false,
     "start_time": "2023-07-02T00:19:59.802797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot styles\n",
    "font_dict_header = {'size': 20, 'weight': 'bold'}\n",
    "font_dict_axistitle = {'size': 14, 'weight': 'bold'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a4bbed9",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36428ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random_seed = 101010\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# create x and y train\n",
    "X_train = train_df.drop(['Id', 'Alpha', 'Class'], axis = 1, inplace = False)\n",
    "y_train_class = train_df['Class']\n",
    "y_train_alpha = train_df['Alpha']\n",
    "alpha_encoder = LabelEncoder()\n",
    "y_train_alpha = alpha_encoder.fit_transform(y_train_alpha)\n",
    "\n",
    "# clean categorical data\n",
    "X_train['EJ'].replace({'A': 0, 'B': 1}, inplace = True)\n",
    "\n",
    "# scale and impute data\n",
    "X_train_columns = X_train.columns\n",
    "X_train_index = X_train.index\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "X_train = standard_scaler.fit_transform(X_train)\n",
    "\n",
    "knn_imputer = KNNImputer()\n",
    "X_train = knn_imputer.fit_transform(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns = X_train_columns, index = X_train_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eec8fc25",
   "metadata": {},
   "source": [
    "# Alpha Prediction Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1efbb28",
   "metadata": {},
   "source": [
    "### Model Prediction Features\n",
    "KNN, Extra Trees, and TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2834aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model_name, model, X_train, y_train, X_test, features):\n",
    "    X = X_train[features].copy()\n",
    "    y = y_train.copy()\n",
    "    test = X_test[features].copy()\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    model_predictions = model.predict_proba(test)\n",
    "    predictions_df = pd.DataFrame(model_predictions, columns = model.classes_, index=X_test.index)\n",
    "    predictions_df.columns = [model_name + '_' + str(col) for col in predictions_df.columns]\n",
    "    \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "684d059d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 5, random_state = random_seed, shuffle = True)\n",
    "model_prediction_features_df = pd.DataFrame({})\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train_alpha)):\n",
    "    fold, oof = X_train.loc[train_index], X_train.loc[test_index]\n",
    "    fold_y, oof_y = y_train_alpha[train_index], y_train_alpha[test_index]\n",
    "    \n",
    "    model_features = [\n",
    "        'DU', 'CR', 'AB', 'DA', 'DH', 'BC', 'FR', 'EP', 'DI', 'FL', 'EU', 'EH', 'Days Since 1-1-2019'\n",
    "    ]\n",
    "    \n",
    "    # KNN\n",
    "    knn_features = model_features[0:3]\n",
    "    knn_features.append(model_features[-1])\n",
    "    knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "    knn_predictions = fit_model('KNN 7', knn, fold, fold_y, oof, knn_features)\n",
    "    \n",
    "    # Extra Trees\n",
    "    extra_trees = ExtraTreesClassifier(n_estimators = 250, random_state = random_seed)\n",
    "    extra_trees_predictions = fit_model('Extra Trees', extra_trees, fold, fold_y, oof, model_features)\n",
    "    \n",
    "    # TabPFN\n",
    "    tabpfn = TabPFNClassifier(N_ensemble_configurations = 64, seed = random_seed)\n",
    "    tabpfn_predictions = fit_model('TabPFN', tabpfn, fold, fold_y, oof, model_features)\n",
    "    \n",
    "    predictions_df = pd.concat([knn_predictions, extra_trees_predictions, tabpfn_predictions], axis = 1)\n",
    "    model_prediction_features_df = pd.concat([model_prediction_features_df, predictions_df])\n",
    "    \n",
    "X_train = pd.concat([X_train, model_prediction_features_df], axis = 1)\n",
    "for pred_class in y_train_alpha:\n",
    "    X_train['Alpha_' + str(pred_class)] = X_train['Extra Trees_' + str(pred_class)] + X_train['TabPFN_' + str(pred_class)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "758d4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_fit_models = {\n",
    "    'KNN 7': knn.fit(X_train[knn_features], y_train_alpha),\n",
    "    'Extra Trees': extra_trees.fit(X_train[model_features], y_train_alpha),\n",
    "    'TabPFN': tabpfn.fit(X_train[model_features], y_train_alpha)\n",
    "}\n",
    "\n",
    "X_train = pd.concat([X_train, model_prediction_features_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac89ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_class in y_train_alpha:\n",
    "    X_train['Alpha_' + str(pred_class)] = (X_train['Extra Trees_' + str(pred_class)] + X_train['TabPFN_' + str(pred_class)]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d287ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (473, 75)\n",
      "Shape of y: (473,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X:', X_train.shape)\n",
    "print('Shape of y:', y_train_alpha.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Shape of X:', X_test.shape)\n",
    "# print('Shape of y:', y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e413c60",
   "metadata": {},
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5026f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def competition_log_loss(y_true, y_pred):\n",
    "    # calculate the predictin probability, clip it to avoid log(0) and calculate the log loss\n",
    "    proba_1 = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    proba_0 = 1 - proba_1\n",
    "    \n",
    "    # count each class\n",
    "    class_0 = np.sum(1 - y_true)\n",
    "    class_1 = np.sum(y_true)\n",
    "\n",
    "    # log loss for each class\n",
    "    log_loss_0 = -np.sum((1 - y_true) * np.log(proba_0)) / class_0\n",
    "    log_loss_1 = -np.sum(y_true * np.log(proba_1)) / class_1\n",
    "    \n",
    "    # return average log loss\n",
    "    return (log_loss_0 + log_loss_1)/2\n",
    "\n",
    "# make scorer for sklearn GridSearchCV\n",
    "balanced_log_loss_scorer = make_scorer(competition_log_loss, greater_is_better = False, needs_proba = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6b6f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Class'] = y_train_class\n",
    "X_train['Alpha'] = y_train_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff109cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Log Loss: 0.26829270112779136\n",
      "Balanced Log Loss: 0.11474936790363675\n",
      "Balanced Log Loss: 0.34167128556765897\n",
      "Balanced Log Loss: 0.45097281300375397\n",
      "Balanced Log Loss: 0.3645385067073838\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "log_loss = []\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = random_seed, shuffle = True)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train_class)):\n",
    "    fold, oof = X_train.loc[train_index], X_train.loc[test_index]\n",
    "    fold_y_class, oof_y_class = y_train_class[train_index], y_train_class[test_index]\n",
    "    fold_y_alpha, oof_y_alpha = y_train_alpha[train_index], y_train_alpha[test_index]\n",
    "    \n",
    "    # over sample\n",
    "    over_sampler = RandomOverSampler(random_state = random_seed)\n",
    "    fold, fold_y_class = over_sampler.fit_resample(fold, fold_y_class)\n",
    "    fold_y_alpha = fold['Alpha']\n",
    "    fold.drop(['Alpha', 'Class'], axis = 1, inplace = True)\n",
    "    oof.drop(['Alpha', 'Class'], axis = 1, inplace = True)\n",
    "    \n",
    "    # pre-process fold data\n",
    "    fold['EJ'].replace({'A': 0, 'B': 1}, inplace = True)\n",
    "    fold_columns = fold.columns\n",
    "    fold_index = fold.index\n",
    "    standard_scaler = StandardScaler()\n",
    "    fold = standard_scaler.fit_transform(fold)\n",
    "    knn_imputer = KNNImputer()\n",
    "    fold = knn_imputer.fit_transform(fold)\n",
    "    fold = pd.DataFrame(fold, columns = fold_columns, index = fold_index)\n",
    "    \n",
    "    # pre-process oof data\n",
    "    oof['EJ'].replace({'A': 0, 'B': 1}, inplace = True)\n",
    "    oof_columns = oof.columns\n",
    "    oof_index = oof.index\n",
    "    oof = standard_scaler.transform(oof)\n",
    "    oof = knn_imputer.transform(oof)\n",
    "    oof = pd.DataFrame(oof, columns = oof_columns, index = oof_index)\n",
    "    \n",
    "    # train model\n",
    "    xgb = XGBClassifier(n_estimators = 250, random_state = random_seed)\n",
    "    # catboost = CatBoostClassifier(iterations = 250, random_state = random_seed, verbose = False)\n",
    "    # xtree = ExtraTreesClassifier(n_estimators = 250, random_state = random_seed)\n",
    "    \n",
    "    xgb.fit(fold, fold_y_alpha)\n",
    "    # catboost.fit(fold, fold_y_alpha)\n",
    "    # xtree.fit(fold, fold_y_alpha)\n",
    "    \n",
    "    # predict oof\n",
    "    oof_y_alpha_proba = xgb.predict_proba(oof)\n",
    "    # oof_y_alpha_proba += catboost.predict_proba(oof)\n",
    "    # oof_y_alpha_proba += xtree.predict_proba(oof)\n",
    "    # oof_y_alpha_proba /= 3\n",
    "    \n",
    "    class_0 = oof_y_alpha_proba[:, 0].sum()\n",
    "    class_123 = oof_y_alpha_proba[:, 1:].sum()\n",
    "    new_probabilities = oof_y_alpha_proba * np.array([[1/(class_0 if i == 0 else class_123) for i in range(oof_y_alpha_proba.shape[1])]])\n",
    "    oof_y_alpha_proba =  new_probabilities / np.sum(new_probabilities, axis = 1, keepdims = 1)\n",
    "    \n",
    "    oof_y_class_proba = oof_y_alpha_proba[:, 1:].sum(axis = 1)\n",
    "    oof_y_class_proba[oof_y_class_proba > 0.98] = 1\n",
    "    oof_y_class_proba[oof_y_class_proba < 0.02] = 0\n",
    "    \n",
    "    fold_log_loss = competition_log_loss(oof_y_class, oof_y_class_proba)\n",
    "    log_loss.append(fold_log_loss)\n",
    "    \n",
    "    print('Balanced Log Loss:', fold_log_loss)\n",
    "    y_true.append(oof_y_class)\n",
    "    y_pred.append(oof_y_class_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5421f67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.11495877e-04, 4.41868460e-04, 5.10996092e-04, 5.52139672e-04,\n",
       "       5.60528545e-04, 6.28487787e-04, 6.41189418e-04, 6.71372669e-04,\n",
       "       7.03905585e-04, 7.30860985e-04, 7.53511396e-04, 7.68150380e-04,\n",
       "       7.91626273e-04, 8.62483973e-04, 8.91497675e-04, 9.38980140e-04,\n",
       "       1.04054981e-03, 1.05760711e-03, 1.09932233e-03, 1.13049396e-03,\n",
       "       1.22748564e-03, 1.22985057e-03, 1.27334774e-03, 1.29113501e-03,\n",
       "       1.32040525e-03, 1.39237173e-03, 1.51814327e-03, 1.61556049e-03,\n",
       "       1.66966832e-03, 1.68219130e-03, 1.69605384e-03, 1.69902179e-03,\n",
       "       1.74199616e-03, 1.79859837e-03, 1.82554866e-03, 1.86909805e-03,\n",
       "       1.87064989e-03, 1.90485472e-03, 1.90901066e-03, 1.96152785e-03,\n",
       "       2.04275679e-03, 2.11596554e-03, 2.16922557e-03, 2.17306024e-03,\n",
       "       2.19619452e-03, 2.32017323e-03, 2.42391971e-03, 2.55144566e-03,\n",
       "       2.61690641e-03, 2.69713629e-03, 2.96355982e-03, 3.15215682e-03,\n",
       "       3.28452825e-03, 3.37311389e-03, 3.40799947e-03, 4.00884845e-03,\n",
       "       4.01924859e-03, 4.29892116e-03, 5.08951585e-03, 5.49884284e-03,\n",
       "       6.71144021e-03, 7.01616930e-03, 7.70301490e-03, 7.72711122e-03,\n",
       "       8.17667011e-03, 8.29149089e-03, 8.73467093e-03, 8.82422148e-03,\n",
       "       1.09596035e-02, 1.20404188e-02, 2.03831618e-02, 2.40906181e-02,\n",
       "       5.28871090e-02, 4.94529387e-01, 8.40038023e-01, 9.73638899e-01,\n",
       "       9.84264501e-01, 9.94111039e-01, 9.96208588e-01, 9.97290277e-01,\n",
       "       9.99230935e-01, 9.99256094e-01, 9.99309562e-01, 9.99449830e-01,\n",
       "       9.99471824e-01, 9.99684681e-01, 9.99727164e-01, 9.99738408e-01,\n",
       "       9.99819852e-01, 9.99834757e-01, 9.99847954e-01, 9.99848467e-01,\n",
       "       9.99861055e-01, 9.99911828e-01])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred[3].sort()\n",
    "y_pred[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 203.946964,
   "end_time": "2023-07-02T00:22:39.276183",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-02T00:19:15.329219",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
