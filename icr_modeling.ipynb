{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import umap\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv('icr-identify-age-related-conditions/train.csv')\n",
    "test_df = pd.read_csv('icr-identify-age-related-conditions/test.csv')\n",
    "greeks_df = pd.read_csv('icr-identify-age-related-conditions/greeks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot styles\n",
    "font_dict_header = {'size': 20, 'weight': 'bold'}\n",
    "font_dict_axistitle = {'size': 14, 'weight': 'bold'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process data\n",
    "y = train_df['Class']\n",
    "\n",
    "x_dummies = pd.get_dummies(train_df[['EJ']])\n",
    "x = pd.concat([train_df, x_dummies], axis = 1)\n",
    "x = x.select_dtypes(include = [np.number])\n",
    "x = x.drop(['Class'], axis = 1, inplace = False)\n",
    "\n",
    "# add na counts\n",
    "for col in x.columns:\n",
    "    x[col + ' NA'] = x[col].isna().astype(int)\n",
    "    \n",
    "all_zero_cols = x.columns[x.max() == 0]\n",
    "x = x.drop(all_zero_cols, axis = 1)\n",
    "\n",
    "# split data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, stratify = y)\n",
    "\n",
    "# build pipeline\n",
    "\n",
    "# normalize data\n",
    "x_train_columns = x_train.columns\n",
    "x_train_index = x_train.index\n",
    "power_scaler = PowerTransformer()\n",
    "x_train = power_scaler.fit_transform(x_train)\n",
    "standard_scaler = StandardScaler()\n",
    "x_train = standard_scaler.fit_transform(x_train)\n",
    "knn_imputer = KNNImputer()\n",
    "x_train = knn_imputer.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train, columns = x_train_columns, index = x_train_index)\n",
    "\n",
    "# dimension reduction\n",
    "pca_n = 3\n",
    "pca = PCA(n_components = pca_n)\n",
    "pca_columns = ['Principal Component ' + str(i + 1) for i in range(pca_n)]\n",
    "principal_components = pca.fit_transform(x_train)\n",
    "pca_df = pd.DataFrame(data = principal_components, columns = pca_columns, index = x_train_index)\n",
    "x_train = pd.concat([x_train, pca_df], axis = 1)\n",
    "\n",
    "umap_n = 3\n",
    "umap_reducer = umap.UMAP(n_components = umap_n)\n",
    "umap_columns = ['Component ' + str(i + 1) for i in range(umap_n)]\n",
    "umap_components = umap_reducer.fit_transform(x_train)\n",
    "umap_df = pd.DataFrame(data = umap_components, columns = umap_columns, index = x_train_index)\n",
    "x_train = pd.concat([x_train, umap_df], axis = 1)\n",
    "\n",
    "# over sample positive class\n",
    "over_sampler = SMOTE()\n",
    "x_train_oversampled, y_train_oversampled = over_sampler.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(x):\n",
    "    x_columns = x.columns\n",
    "    x_index = x.index\n",
    "    \n",
    "    x = power_scaler.transform(x)\n",
    "    x = standard_scaler.transform(x)\n",
    "    x = knn_imputer.transform(x)\n",
    "    x = pd.DataFrame(x, columns = x_columns, index = x_index)\n",
    "    \n",
    "    principal_components = pca.transform(x)\n",
    "    pca_df = pd.DataFrame(data = principal_components, columns = pca_columns, index = x_index)\n",
    "    x = pd.concat([x, pca_df], axis = 1)\n",
    "    \n",
    "    umap_components = umap_reducer.transform(x)\n",
    "    umap_df = pd.DataFrame(data = umap_components, columns = umap_columns, index = x_index)\n",
    "    x = pd.concat([x, umap_df], axis = 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = preprocess_pipeline(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature training set dimensions:  (814, 70)\n",
      "Feature test set dimensions: (124, 70)\n",
      "\n",
      "Responce distribution training set:\n",
      "Class\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Responce distribution test set:\n",
      "Class\n",
      "0    0.822581\n",
      "1    0.177419\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# printing details \n",
    "print(\"Feature training set dimensions: \", x_train_oversampled.shape)   \n",
    "print(\"Feature test set dimensions:\", x_val.shape)\n",
    "print()\n",
    "print(\"Responce distribution training set:\")\n",
    "print(y_train_oversampled.value_counts(normalize = True))\n",
    "print()\n",
    "print(\"Responce distribution test set:\")\n",
    "print(y_val.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}\n",
    "models_dict['power_scaler'] = power_scaler\n",
    "models_dict['standard_scaler'] = standard_scaler\n",
    "models_dict['knn_imputer'] = knn_imputer\n",
    "models_dict['pca'] = pca\n",
    "models_dict['umap_reducer'] = umap_reducer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(y_true, y_pred):\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    return roc_auc, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(df, row, n_neighbors):\n",
    "    if row > len(df):\n",
    "        raise ValueError('Row index is greater than the number of rows in the dataframe.')\n",
    "    \n",
    "    if row < 0:\n",
    "        raise ValueError('Row index is less than 0.')\n",
    "    \n",
    "    knn_df = df.drop('Class', axis = 1)\n",
    "    knn_columns = knn_df.columns\n",
    "    knn_index = knn_df.index\n",
    "    knn_df = StandardScaler().fit_transform(knn_df)\n",
    "    knn_df = pd.DataFrame(knn_df, columns = knn_columns, index = knn_index)\n",
    "    knn = NearestNeighbors(n_neighbors = n_neighbors).fit(knn_df)\n",
    "    \n",
    "    distances, indices = knn.kneighbors(knn_df.loc[row, :].values.reshape(1, -1))\n",
    "    \n",
    "    distances = np.array(distances)\n",
    "    neighbors = distances.argsort()[:n_neighbors]\n",
    "    \n",
    "    return df.iloc[indices[0], :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=0.2, n_estimators=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.2, n_estimators=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.2, n_estimators=500)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'loss': ['log_loss'],\n",
    "    'learning_rate': [0.2],\n",
    "    'n_estimators': [500],\n",
    "    'criterion': ['friedman_mse'],\n",
    "    'max_depth': [3]\n",
    "}\n",
    "\n",
    "grid_obj = GridSearchCV(gb, parameters, scoring = 'roc_auc', cv = 5)\n",
    "grid_obj = grid_obj.fit(x_train_oversampled, y_train_oversampled)\n",
    "gb = grid_obj.best_estimator_\n",
    "gb.fit(x_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(gb, 'gb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.272823</td>\n",
       "      <td>0.135727</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.2</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'friedman_mse', 'learning_rate':...</td>\n",
       "      <td>0.988407</td>\n",
       "      <td>0.997139</td>\n",
       "      <td>0.997441</td>\n",
       "      <td>0.998193</td>\n",
       "      <td>0.987197</td>\n",
       "      <td>0.993675</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       2.272823      0.135727         0.002896        0.000079   \n",
       "\n",
       "  param_criterion param_learning_rate param_loss param_max_depth  \\\n",
       "0    friedman_mse                 0.2   log_loss               3   \n",
       "\n",
       "  param_n_estimators                                             params  \\\n",
       "0                500  {'criterion': 'friedman_mse', 'learning_rate':...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.988407           0.997139           0.997441           0.998193   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.987197         0.993675        0.004823                1  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view results of grid search\n",
    "gb_results = pd.DataFrame(grid_obj.cv_results_)\n",
    "gb_results.sort_values('rank_test_score').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.93\n",
      "Accuracy: 0.97\n",
      "Precision: 0.95\n",
      "Recall: 0.86\n",
      "F1 Score: 0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9269162210338682,\n",
       " 0.967741935483871,\n",
       " 0.95,\n",
       " 0.8636363636363636,\n",
       " 0.9047619047619048)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_performance(y_val, gb.predict(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a pandas series with features importances and sort it\n",
    "importances = pd.Series(gb.feature_importances_, index = x_train_oversampled.columns).sort_values(ascending = True)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize = (10, 20))\n",
    "plt.title('Feature Importances', fontdict = font_dict_header)\n",
    "plt.box(False)\n",
    "plt.barh(importances.index, importances.values)\n",
    "plt.xlabel('Importance Score', fontdict = font_dict_axistitle)\n",
    "plt.ylabel('Features', fontdict = font_dict_axistitle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict['gb'] = gb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = to_categorical(y_train_oversampled)\n",
    "y_val_encoded = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 22:01:33.032752: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8428"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 22:01:35.025779: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.90416, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 3s 65ms/step - loss: 0.3914 - accuracy: 0.8428 - val_loss: 0.9042 - val_accuracy: 0.6452\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.2168 - accuracy: 0.9275\n",
      "Epoch 2: val_loss improved from 0.90416 to 0.66098, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 0.2168 - accuracy: 0.9275 - val_loss: 0.6610 - val_accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.1495 - accuracy: 0.9533\n",
      "Epoch 3: val_loss improved from 0.66098 to 0.42378, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 0.1495 - accuracy: 0.9533 - val_loss: 0.4238 - val_accuracy: 0.8226\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.9484\n",
      "Epoch 4: val_loss improved from 0.42378 to 0.36996, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 1s 20ms/step - loss: 0.1587 - accuracy: 0.9484 - val_loss: 0.3700 - val_accuracy: 0.8387\n",
      "Epoch 5/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.1342 - accuracy: 0.9588\n",
      "Epoch 5: val_loss improved from 0.36996 to 0.35915, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 0s 19ms/step - loss: 0.1321 - accuracy: 0.9595 - val_loss: 0.3591 - val_accuracy: 0.8710\n",
      "Epoch 6/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.1492 - accuracy: 0.9513\n",
      "Epoch 6: val_loss did not improve from 0.35915\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1574 - accuracy: 0.9484 - val_loss: 0.3836 - val_accuracy: 0.8710\n",
      "Epoch 7/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.1463 - accuracy: 0.9463\n",
      "Epoch 7: val_loss improved from 0.35915 to 0.33935, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.1514 - accuracy: 0.9447 - val_loss: 0.3394 - val_accuracy: 0.8952\n",
      "Epoch 8/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.1444 - accuracy: 0.9475\n",
      "Epoch 8: val_loss improved from 0.33935 to 0.28846, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1427 - accuracy: 0.9484 - val_loss: 0.2885 - val_accuracy: 0.9032\n",
      "Epoch 9/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.1090 - accuracy: 0.9650\n",
      "Epoch 9: val_loss did not improve from 0.28846\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1097 - accuracy: 0.9644 - val_loss: 0.3771 - val_accuracy: 0.8790\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9693\n",
      "Epoch 10: val_loss improved from 0.28846 to 0.25369, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 1s 22ms/step - loss: 0.0979 - accuracy: 0.9693 - val_loss: 0.2537 - val_accuracy: 0.8871\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9779\n",
      "Epoch 11: val_loss did not improve from 0.25369\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0845 - accuracy: 0.9779 - val_loss: 0.2545 - val_accuracy: 0.9032\n",
      "Epoch 12/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0696 - accuracy: 0.9800\n",
      "Epoch 12: val_loss did not improve from 0.25369\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0685 - accuracy: 0.9803 - val_loss: 0.3550 - val_accuracy: 0.8871\n",
      "Epoch 13/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0907 - accuracy: 0.9725\n",
      "Epoch 13: val_loss did not improve from 0.25369\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.0895 - accuracy: 0.9730 - val_loss: 0.2751 - val_accuracy: 0.9032\n",
      "Epoch 14/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0738 - accuracy: 0.9792\n",
      "Epoch 14: val_loss improved from 0.25369 to 0.23829, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.0737 - accuracy: 0.9791 - val_loss: 0.2383 - val_accuracy: 0.9113\n",
      "Epoch 15/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0920 - accuracy: 0.9688\n",
      "Epoch 15: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0917 - accuracy: 0.9681 - val_loss: 0.3614 - val_accuracy: 0.8871\n",
      "Epoch 16/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.1032 - accuracy: 0.9688\n",
      "Epoch 16: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1018 - accuracy: 0.9693 - val_loss: 0.3037 - val_accuracy: 0.9032\n",
      "Epoch 17/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0460 - accuracy: 0.9825\n",
      "Epoch 17: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0454 - accuracy: 0.9828 - val_loss: 0.2692 - val_accuracy: 0.9032\n",
      "Epoch 18/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0521 - accuracy: 0.9837\n",
      "Epoch 18: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0617 - accuracy: 0.9816 - val_loss: 0.5279 - val_accuracy: 0.8629\n",
      "Epoch 19/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0731 - accuracy: 0.9750\n",
      "Epoch 19: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0720 - accuracy: 0.9754 - val_loss: 0.3170 - val_accuracy: 0.9113\n",
      "Epoch 20/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0860 - accuracy: 0.9688\n",
      "Epoch 20: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0848 - accuracy: 0.9693 - val_loss: 0.3363 - val_accuracy: 0.8952\n",
      "Epoch 21/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0748 - accuracy: 0.9787\n",
      "Epoch 21: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0759 - accuracy: 0.9779 - val_loss: 0.4563 - val_accuracy: 0.8790\n",
      "Epoch 22/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0549 - accuracy: 0.9800\n",
      "Epoch 22: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0541 - accuracy: 0.9803 - val_loss: 0.4525 - val_accuracy: 0.8629\n",
      "Epoch 23/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0311 - accuracy: 0.9900\n",
      "Epoch 23: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0307 - accuracy: 0.9902 - val_loss: 0.4763 - val_accuracy: 0.8710\n",
      "Epoch 24/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0354 - accuracy: 0.9925\n",
      "Epoch 24: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0361 - accuracy: 0.9914 - val_loss: 0.4240 - val_accuracy: 0.8871\n",
      "Epoch 25/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0818 - accuracy: 0.9740\n",
      "Epoch 25: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0819 - accuracy: 0.9730 - val_loss: 0.3671 - val_accuracy: 0.8952\n",
      "Epoch 26/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0472 - accuracy: 0.9837\n",
      "Epoch 26: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0470 - accuracy: 0.9840 - val_loss: 0.2580 - val_accuracy: 0.9194\n",
      "Epoch 27/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0526 - accuracy: 0.9844\n",
      "Epoch 27: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0547 - accuracy: 0.9840 - val_loss: 0.4135 - val_accuracy: 0.8871\n",
      "Epoch 28/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0778 - accuracy: 0.9792\n",
      "Epoch 28: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0738 - accuracy: 0.9803 - val_loss: 0.3158 - val_accuracy: 0.8871\n",
      "Epoch 29/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0598 - accuracy: 0.9787\n",
      "Epoch 29: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0592 - accuracy: 0.9791 - val_loss: 0.4213 - val_accuracy: 0.8710\n",
      "Epoch 30/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0590 - accuracy: 0.9825\n",
      "Epoch 30: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0583 - accuracy: 0.9828 - val_loss: 0.2727 - val_accuracy: 0.9194\n",
      "Epoch 31/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0526 - accuracy: 0.9831\n",
      "Epoch 31: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0510 - accuracy: 0.9828 - val_loss: 0.3266 - val_accuracy: 0.9032\n",
      "Epoch 32/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0690 - accuracy: 0.9818\n",
      "Epoch 32: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0675 - accuracy: 0.9816 - val_loss: 0.2775 - val_accuracy: 0.9274\n",
      "Epoch 33/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0600 - accuracy: 0.9753\n",
      "Epoch 33: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0573 - accuracy: 0.9767 - val_loss: 0.2395 - val_accuracy: 0.9113\n",
      "Epoch 34/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0260 - accuracy: 0.9883\n",
      "Epoch 34: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0291 - accuracy: 0.9877 - val_loss: 0.2683 - val_accuracy: 0.9194\n",
      "Epoch 35/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0386 - accuracy: 0.9844\n",
      "Epoch 35: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0378 - accuracy: 0.9840 - val_loss: 0.2705 - val_accuracy: 0.9194\n",
      "Epoch 36/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0413 - accuracy: 0.9851\n",
      "Epoch 36: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0411 - accuracy: 0.9840 - val_loss: 0.3114 - val_accuracy: 0.9194\n",
      "Epoch 37/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0609 - accuracy: 0.9805\n",
      "Epoch 37: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0585 - accuracy: 0.9816 - val_loss: 0.4016 - val_accuracy: 0.9113\n",
      "Epoch 38/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0417 - accuracy: 0.9831\n",
      "Epoch 38: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0428 - accuracy: 0.9828 - val_loss: 0.3135 - val_accuracy: 0.9032\n",
      "Epoch 39/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0563 - accuracy: 0.9810\n",
      "Epoch 39: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0553 - accuracy: 0.9803 - val_loss: 0.3714 - val_accuracy: 0.8871\n",
      "Epoch 40/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0335 - accuracy: 0.9896\n",
      "Epoch 40: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0350 - accuracy: 0.9889 - val_loss: 0.4420 - val_accuracy: 0.8790\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9877\n",
      "Epoch 41: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0419 - accuracy: 0.9877 - val_loss: 0.3424 - val_accuracy: 0.8952\n",
      "Epoch 42/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0378 - accuracy: 0.9851\n",
      "Epoch 42: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0457 - accuracy: 0.9828 - val_loss: 0.4161 - val_accuracy: 0.8790\n",
      "Epoch 43/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0403 - accuracy: 0.9896\n",
      "Epoch 43: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0574 - accuracy: 0.9853 - val_loss: 0.4233 - val_accuracy: 0.8790\n",
      "Epoch 44/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0360 - accuracy: 0.9909\n",
      "Epoch 44: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0347 - accuracy: 0.9914 - val_loss: 0.5656 - val_accuracy: 0.8871\n",
      "Epoch 45/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0691 - accuracy: 0.9787\n",
      "Epoch 45: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0682 - accuracy: 0.9791 - val_loss: 0.2710 - val_accuracy: 0.9032\n",
      "Epoch 46/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0467 - accuracy: 0.9875\n",
      "Epoch 46: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0462 - accuracy: 0.9877 - val_loss: 0.3014 - val_accuracy: 0.9032\n",
      "Epoch 47/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0448 - accuracy: 0.9851\n",
      "Epoch 47: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0521 - accuracy: 0.9828 - val_loss: 0.3421 - val_accuracy: 0.8871\n",
      "Epoch 48/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0306 - accuracy: 0.9891\n",
      "Epoch 48: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0296 - accuracy: 0.9902 - val_loss: 0.3706 - val_accuracy: 0.8952\n",
      "Epoch 49/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0230 - accuracy: 0.9948\n",
      "Epoch 49: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 0.3372 - val_accuracy: 0.8871\n",
      "Epoch 50/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0273 - accuracy: 0.9922\n",
      "Epoch 50: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0281 - accuracy: 0.9914 - val_loss: 0.4642 - val_accuracy: 0.8629\n",
      "Epoch 51/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0283 - accuracy: 0.9909\n",
      "Epoch 51: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0272 - accuracy: 0.9914 - val_loss: 0.4304 - val_accuracy: 0.8710\n",
      "Epoch 52/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0213 - accuracy: 0.9948\n",
      "Epoch 52: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0288 - accuracy: 0.9939 - val_loss: 0.4276 - val_accuracy: 0.8790\n",
      "Epoch 53/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0362 - accuracy: 0.9900\n",
      "Epoch 53: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0362 - accuracy: 0.9902 - val_loss: 0.3062 - val_accuracy: 0.9113\n",
      "Epoch 54/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0459 - accuracy: 0.9857\n",
      "Epoch 54: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0435 - accuracy: 0.9865 - val_loss: 0.4320 - val_accuracy: 0.9194\n",
      "Epoch 55/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0158 - accuracy: 0.9946\n",
      "Epoch 55: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.4690 - val_accuracy: 0.9113\n",
      "Epoch 56/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0221 - accuracy: 0.9975\n",
      "Epoch 56: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0218 - accuracy: 0.9975 - val_loss: 0.3721 - val_accuracy: 0.9113\n",
      "Epoch 57/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0474 - accuracy: 0.9810\n",
      "Epoch 57: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0477 - accuracy: 0.9816 - val_loss: 0.3342 - val_accuracy: 0.9113\n",
      "Epoch 58/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0360 - accuracy: 0.9875\n",
      "Epoch 58: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0420 - accuracy: 0.9840 - val_loss: 0.3370 - val_accuracy: 0.9032\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9840\n",
      "Epoch 59: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0569 - accuracy: 0.9840 - val_loss: 0.3170 - val_accuracy: 0.9113\n",
      "Epoch 60/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0354 - accuracy: 0.9896\n",
      "Epoch 60: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0336 - accuracy: 0.9902 - val_loss: 0.3278 - val_accuracy: 0.9113\n",
      "Epoch 61/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0225 - accuracy: 0.9935\n",
      "Epoch 61: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0225 - accuracy: 0.9939 - val_loss: 0.3457 - val_accuracy: 0.8952\n",
      "Epoch 62/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0255 - accuracy: 0.9937\n",
      "Epoch 62: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0251 - accuracy: 0.9939 - val_loss: 0.3606 - val_accuracy: 0.8952\n",
      "Epoch 63/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0138 - accuracy: 0.9948\n",
      "Epoch 63: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0136 - accuracy: 0.9951 - val_loss: 0.3964 - val_accuracy: 0.8952\n",
      "Epoch 64/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0401 - accuracy: 0.9857\n",
      "Epoch 64: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0381 - accuracy: 0.9865 - val_loss: 0.3343 - val_accuracy: 0.9032\n",
      "Epoch 65/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0196 - accuracy: 0.9935\n",
      "Epoch 65: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.3096 - val_accuracy: 0.9113\n",
      "Epoch 66/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0409 - accuracy: 0.9837\n",
      "Epoch 66: val_loss did not improve from 0.23829\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0559 - accuracy: 0.9791 - val_loss: 0.3232 - val_accuracy: 0.9113\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9853\n",
      "Epoch 67: val_loss improved from 0.23829 to 0.21293, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0383 - accuracy: 0.9853 - val_loss: 0.2129 - val_accuracy: 0.9274\n",
      "Epoch 68/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0286 - accuracy: 0.9909\n",
      "Epoch 68: val_loss did not improve from 0.21293\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0332 - accuracy: 0.9889 - val_loss: 0.2615 - val_accuracy: 0.9274\n",
      "Epoch 69/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0435 - accuracy: 0.9883\n",
      "Epoch 69: val_loss did not improve from 0.21293\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0499 - accuracy: 0.9865 - val_loss: 0.3252 - val_accuracy: 0.9032\n",
      "Epoch 70/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0622 - accuracy: 0.9742\n",
      "Epoch 70: val_loss improved from 0.21293 to 0.16848, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0604 - accuracy: 0.9754 - val_loss: 0.1685 - val_accuracy: 0.9597\n",
      "Epoch 71/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0392 - accuracy: 0.9878\n",
      "Epoch 71: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0455 - accuracy: 0.9865 - val_loss: 0.2904 - val_accuracy: 0.9274\n",
      "Epoch 72/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0437 - accuracy: 0.9905\n",
      "Epoch 72: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0400 - accuracy: 0.9914 - val_loss: 0.2997 - val_accuracy: 0.9194\n",
      "Epoch 73/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0327 - accuracy: 0.9905\n",
      "Epoch 73: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0436 - accuracy: 0.9865 - val_loss: 0.2711 - val_accuracy: 0.9274\n",
      "Epoch 74/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0412 - accuracy: 0.9857\n",
      "Epoch 74: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0405 - accuracy: 0.9865 - val_loss: 0.3986 - val_accuracy: 0.8871\n",
      "Epoch 75/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0286 - accuracy: 0.9935\n",
      "Epoch 75: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0364 - accuracy: 0.9914 - val_loss: 0.6566 - val_accuracy: 0.8548\n",
      "Epoch 76/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0693 - accuracy: 0.9762\n",
      "Epoch 76: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0754 - accuracy: 0.9754 - val_loss: 0.2937 - val_accuracy: 0.9274\n",
      "Epoch 77/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0196 - accuracy: 0.9959\n",
      "Epoch 77: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.0187 - accuracy: 0.9963 - val_loss: 0.3081 - val_accuracy: 0.9113\n",
      "Epoch 78/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0569 - accuracy: 0.9818\n",
      "Epoch 78: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0661 - accuracy: 0.9803 - val_loss: 0.2848 - val_accuracy: 0.9274\n",
      "Epoch 79/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0352 - accuracy: 0.9887\n",
      "Epoch 79: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0348 - accuracy: 0.9889 - val_loss: 0.3127 - val_accuracy: 0.9274\n",
      "Epoch 80/100\n",
      "22/26 [========================>.....] - ETA: 0s - loss: 0.0270 - accuracy: 0.9886\n",
      "Epoch 80: val_loss did not improve from 0.16848\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0318 - accuracy: 0.9865 - val_loss: 0.3364 - val_accuracy: 0.9194\n",
      "Epoch 81/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0792 - accuracy: 0.9750\n",
      "Epoch 81: val_loss improved from 0.16848 to 0.13917, saving model to model_weights_best.h5\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.0785 - accuracy: 0.9754 - val_loss: 0.1392 - val_accuracy: 0.9677\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9877\n",
      "Epoch 82: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0463 - accuracy: 0.9877 - val_loss: 0.1752 - val_accuracy: 0.9435\n",
      "Epoch 83/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0332 - accuracy: 0.9900\n",
      "Epoch 83: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0328 - accuracy: 0.9902 - val_loss: 0.2311 - val_accuracy: 0.9355\n",
      "Epoch 84/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0461 - accuracy: 0.9862\n",
      "Epoch 84: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0458 - accuracy: 0.9865 - val_loss: 0.2239 - val_accuracy: 0.9274\n",
      "Epoch 85/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0243 - accuracy: 0.9925\n",
      "Epoch 85: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.2561 - val_accuracy: 0.9355\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9939\n",
      "Epoch 86: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.2748 - val_accuracy: 0.9113\n",
      "Epoch 87/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0371 - accuracy: 0.9870\n",
      "Epoch 87: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0375 - accuracy: 0.9865 - val_loss: 0.3199 - val_accuracy: 0.9194\n",
      "Epoch 88/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 0.9909\n",
      "Epoch 88: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0338 - accuracy: 0.9902 - val_loss: 0.3860 - val_accuracy: 0.8952\n",
      "Epoch 89/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0409 - accuracy: 0.9870\n",
      "Epoch 89: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0472 - accuracy: 0.9865 - val_loss: 0.2790 - val_accuracy: 0.9194\n",
      "Epoch 90/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.1037 - accuracy: 0.9727\n",
      "Epoch 90: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1000 - accuracy: 0.9730 - val_loss: 0.2620 - val_accuracy: 0.9194\n",
      "Epoch 91/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0465 - accuracy: 0.9922\n",
      "Epoch 91: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0453 - accuracy: 0.9926 - val_loss: 0.2126 - val_accuracy: 0.9355\n",
      "Epoch 92/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0517 - accuracy: 0.9810\n",
      "Epoch 92: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0470 - accuracy: 0.9828 - val_loss: 0.1824 - val_accuracy: 0.9597\n",
      "Epoch 93/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0339 - accuracy: 0.9883\n",
      "Epoch 93: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0375 - accuracy: 0.9853 - val_loss: 0.2074 - val_accuracy: 0.9435\n",
      "Epoch 94/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.0549 - accuracy: 0.9850\n",
      "Epoch 94: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0540 - accuracy: 0.9853 - val_loss: 0.2932 - val_accuracy: 0.8871\n",
      "Epoch 95/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0261 - accuracy: 0.9922\n",
      "Epoch 95: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0248 - accuracy: 0.9926 - val_loss: 0.2999 - val_accuracy: 0.8952\n",
      "Epoch 96/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0291 - accuracy: 0.9909\n",
      "Epoch 96: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0304 - accuracy: 0.9902 - val_loss: 0.2673 - val_accuracy: 0.9274\n",
      "Epoch 97/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0561 - accuracy: 0.9878\n",
      "Epoch 97: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0513 - accuracy: 0.9889 - val_loss: 0.2724 - val_accuracy: 0.9355\n",
      "Epoch 98/100\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.0213 - accuracy: 0.9932\n",
      "Epoch 98: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.2731 - val_accuracy: 0.9194\n",
      "Epoch 99/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0198 - accuracy: 0.9935\n",
      "Epoch 99: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.2859 - val_accuracy: 0.9113\n",
      "Epoch 100/100\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.0273 - accuracy: 0.9935\n",
      "Epoch 100: val_loss did not improve from 0.13917\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0421 - accuracy: 0.9877 - val_loss: 0.2967 - val_accuracy: 0.9113\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0327 - accuracy: 0.9889 - val_loss: 0.2868 - val_accuracy: 0.9194\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2974 - val_accuracy: 0.9113\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0262 - accuracy: 0.9914 - val_loss: 0.2459 - val_accuracy: 0.9274\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0480 - accuracy: 0.9853 - val_loss: 0.3356 - val_accuracy: 0.9194\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.3331 - val_accuracy: 0.9113\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0499 - accuracy: 0.9853 - val_loss: 0.2584 - val_accuracy: 0.9355\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0228 - accuracy: 0.9951 - val_loss: 0.2555 - val_accuracy: 0.9113\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0408 - accuracy: 0.9828 - val_loss: 0.4668 - val_accuracy: 0.8790\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0251 - accuracy: 0.9914 - val_loss: 0.4193 - val_accuracy: 0.8952\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0283 - accuracy: 0.9914 - val_loss: 0.3835 - val_accuracy: 0.9194\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0158 - accuracy: 0.9902 - val_loss: 0.3245 - val_accuracy: 0.9113\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.4275 - val_accuracy: 0.9194\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0501 - accuracy: 0.9816 - val_loss: 0.3130 - val_accuracy: 0.9194\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0150 - accuracy: 0.9963 - val_loss: 0.3011 - val_accuracy: 0.9435\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0591 - accuracy: 0.9828 - val_loss: 0.3274 - val_accuracy: 0.8952\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0511 - accuracy: 0.9791 - val_loss: 0.2400 - val_accuracy: 0.9032\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0416 - accuracy: 0.9877 - val_loss: 0.4506 - val_accuracy: 0.8952\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0586 - accuracy: 0.9840 - val_loss: 0.4172 - val_accuracy: 0.9032\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0211 - accuracy: 0.9926 - val_loss: 0.4781 - val_accuracy: 0.9032\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0256 - accuracy: 0.9902 - val_loss: 0.3564 - val_accuracy: 0.9194\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0446 - accuracy: 0.9902 - val_loss: 0.3341 - val_accuracy: 0.9194\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0492 - accuracy: 0.9877 - val_loss: 0.2887 - val_accuracy: 0.9355\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0323 - accuracy: 0.9889 - val_loss: 0.3166 - val_accuracy: 0.9274\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0521 - accuracy: 0.9853 - val_loss: 0.3124 - val_accuracy: 0.9274\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0618 - accuracy: 0.9828 - val_loss: 0.3052 - val_accuracy: 0.9194\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0496 - accuracy: 0.9853 - val_loss: 0.3505 - val_accuracy: 0.8871\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0670 - accuracy: 0.9865 - val_loss: 0.2950 - val_accuracy: 0.9113\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0329 - accuracy: 0.9902 - val_loss: 0.3023 - val_accuracy: 0.9194\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0254 - accuracy: 0.9939 - val_loss: 0.2821 - val_accuracy: 0.9194\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0217 - accuracy: 0.9939 - val_loss: 0.3005 - val_accuracy: 0.9113\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0287 - accuracy: 0.9939 - val_loss: 0.3748 - val_accuracy: 0.9194\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0311 - accuracy: 0.9877 - val_loss: 0.3457 - val_accuracy: 0.8952\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0319 - accuracy: 0.9889 - val_loss: 0.2812 - val_accuracy: 0.9194\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0344 - accuracy: 0.9926 - val_loss: 0.2685 - val_accuracy: 0.9194\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0327 - accuracy: 0.9889 - val_loss: 0.2102 - val_accuracy: 0.9355\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 0.2295 - val_accuracy: 0.9274\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0426 - accuracy: 0.9877 - val_loss: 0.6480 - val_accuracy: 0.8629\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0724 - accuracy: 0.9828 - val_loss: 0.3963 - val_accuracy: 0.8952\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0255 - accuracy: 0.9963 - val_loss: 0.4251 - val_accuracy: 0.9113\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0260 - accuracy: 0.9914 - val_loss: 0.3147 - val_accuracy: 0.9194\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 0.3589 - val_accuracy: 0.9032\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0331 - accuracy: 0.9926 - val_loss: 0.2741 - val_accuracy: 0.9274\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0706 - accuracy: 0.9779 - val_loss: 0.2854 - val_accuracy: 0.9274\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0235 - accuracy: 0.9951 - val_loss: 0.3367 - val_accuracy: 0.8952\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0269 - accuracy: 0.9902 - val_loss: 0.3066 - val_accuracy: 0.9194\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0345 - accuracy: 0.9877 - val_loss: 0.2170 - val_accuracy: 0.9516\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 0.3239 - val_accuracy: 0.8871\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0274 - accuracy: 0.9889 - val_loss: 0.2492 - val_accuracy: 0.9435\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0167 - accuracy: 0.9963 - val_loss: 0.3013 - val_accuracy: 0.9274\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0329 - accuracy: 0.9914 - val_loss: 0.2901 - val_accuracy: 0.9355\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 0.3230 - val_accuracy: 0.9274\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.3074 - val_accuracy: 0.9194\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0142 - accuracy: 0.9963 - val_loss: 0.3300 - val_accuracy: 0.9194\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.4097 - val_accuracy: 0.9274\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 0.9963 - val_loss: 0.5414 - val_accuracy: 0.8952\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0423 - accuracy: 0.9889 - val_loss: 0.3261 - val_accuracy: 0.9435\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0100 - accuracy: 0.9951 - val_loss: 0.2931 - val_accuracy: 0.9435\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.2353 - val_accuracy: 0.9435\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0448 - accuracy: 0.9865 - val_loss: 0.2298 - val_accuracy: 0.9516\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0210 - accuracy: 0.9951 - val_loss: 0.2193 - val_accuracy: 0.9435\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0262 - accuracy: 0.9914 - val_loss: 0.2129 - val_accuracy: 0.9355\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0296 - accuracy: 0.9902 - val_loss: 0.2361 - val_accuracy: 0.9355\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0084 - accuracy: 0.9963 - val_loss: 0.3229 - val_accuracy: 0.9194\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0256 - accuracy: 0.9963 - val_loss: 0.3964 - val_accuracy: 0.9113\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0276 - accuracy: 0.9877 - val_loss: 0.2542 - val_accuracy: 0.9355\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0218 - accuracy: 0.9902 - val_loss: 0.4070 - val_accuracy: 0.9113\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0137 - accuracy: 0.9963 - val_loss: 0.3824 - val_accuracy: 0.9194\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.3180 - val_accuracy: 0.9274\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0206 - accuracy: 0.9914 - val_loss: 0.3013 - val_accuracy: 0.9355\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0328 - accuracy: 0.9889 - val_loss: 0.2834 - val_accuracy: 0.9194\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0285 - accuracy: 0.9889 - val_loss: 0.2975 - val_accuracy: 0.9435\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0159 - accuracy: 0.9939 - val_loss: 0.2778 - val_accuracy: 0.9274\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.9975 - val_loss: 0.3555 - val_accuracy: 0.9274\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0226 - accuracy: 0.9951 - val_loss: 0.4090 - val_accuracy: 0.9113\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0328 - accuracy: 0.9889 - val_loss: 0.4416 - val_accuracy: 0.9113\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0334 - accuracy: 0.9902 - val_loss: 0.3600 - val_accuracy: 0.9113\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0650 - accuracy: 0.9803 - val_loss: 0.3026 - val_accuracy: 0.9355\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0281 - accuracy: 0.9877 - val_loss: 0.3500 - val_accuracy: 0.9194\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0160 - accuracy: 0.9951 - val_loss: 0.3005 - val_accuracy: 0.9355\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0721 - accuracy: 0.9730 - val_loss: 0.3012 - val_accuracy: 0.9194\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0583 - accuracy: 0.9803 - val_loss: 0.4800 - val_accuracy: 0.9113\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0398 - accuracy: 0.9902 - val_loss: 0.4672 - val_accuracy: 0.9113\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0349 - accuracy: 0.9914 - val_loss: 0.4546 - val_accuracy: 0.9032\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0244 - accuracy: 0.9914 - val_loss: 0.4291 - val_accuracy: 0.9194\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0746 - accuracy: 0.9840 - val_loss: 0.3131 - val_accuracy: 0.9194\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0294 - accuracy: 0.9889 - val_loss: 0.3142 - val_accuracy: 0.9274\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0444 - accuracy: 0.9840 - val_loss: 0.3570 - val_accuracy: 0.9032\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 0.3542 - val_accuracy: 0.9113\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.3091 - val_accuracy: 0.9355\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0251 - accuracy: 0.9951 - val_loss: 0.3928 - val_accuracy: 0.9113\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 0.9963 - val_loss: 0.3077 - val_accuracy: 0.9274\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0139 - accuracy: 0.9939 - val_loss: 0.3316 - val_accuracy: 0.9355\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0287 - accuracy: 0.9914 - val_loss: 0.2866 - val_accuracy: 0.9435\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0303 - accuracy: 0.9926 - val_loss: 0.2927 - val_accuracy: 0.9194\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0170 - accuracy: 0.9951 - val_loss: 0.3721 - val_accuracy: 0.9274\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0469 - accuracy: 0.9877 - val_loss: 0.2420 - val_accuracy: 0.9355\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0227 - accuracy: 0.9939 - val_loss: 0.2584 - val_accuracy: 0.9274\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0439 - accuracy: 0.9865 - val_loss: 0.1984 - val_accuracy: 0.9435\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.0271 - accuracy: 0.9939 - val_loss: 0.1902 - val_accuracy: 0.9355\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0205 - accuracy: 0.9914 - val_loss: 0.1419 - val_accuracy: 0.9597\n",
      "4/4 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 22:02:49.174481: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add an input layer (matches the number of features in your data)\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (x_train_oversampled.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add one hidden layer\n",
    "model.add(Dense(32, activation = 'tanh'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add one hidden layer\n",
    "model.add(Dense(16, activation = 'tanh'))\n",
    "\n",
    "# Add one hidden layer\n",
    "model.add(Dense(8, activation = 'tanh'))\n",
    "\n",
    "# Add an output layer with one neuron and sigmoid activation function (for binary classification)\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = Adam(learning_rate = 0.01), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Create a callback to save the model's weights\n",
    "checkpoint = ModelCheckpoint('model_weights_best.h5',  # where to save the weights\n",
    "                             monitor='val_loss',  # quantity to monitor\n",
    "                             save_best_only=True,  # only save the best weights\n",
    "                             mode='min',  # mode='min' for minimizing loss\n",
    "                             verbose=1)  # print out updates\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_oversampled, y_train_encoded, \n",
    "          epochs=100, \n",
    "          batch_size=32, \n",
    "          verbose=1, \n",
    "          validation_data=(x_val, y_val_encoded), \n",
    "          callbacks=[checkpoint]) \n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_oversampled, y_train_encoded, epochs = 100, batch_size = 32, verbose = 1, validation_data = (x_val, y_val_encoded))\n",
    "\n",
    "# Predict on validation set\n",
    "predictions = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.92\n",
      "Accuracy: 0.96\n",
      "Precision: 0.90\n",
      "Recall: 0.86\n",
      "F1 Score: 0.88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9220142602495544,\n",
       " 0.9596774193548387,\n",
       " 0.9047619047619048,\n",
       " 0.8636363636363636,\n",
       " 0.8837209302325582)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_performance(y_val, np.argmax(predictions, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict['nn'] = model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = []\n",
    "class_1 = []\n",
    "prediction = []\n",
    "for row in gb.predict_proba(x_val):\n",
    "    class_0.append(row[0])\n",
    "    class_1.append(row[1])\n",
    "    prediction.append(np.argmax(row))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_check = x_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_check['class_0'] = class_0\n",
    "x_val_check['class_1'] = class_1\n",
    "x_val_check['prediction'] = prediction \n",
    "\n",
    "x_val_check = x_val_check[x_val_check['prediction'] != y_val].sort_values(by = 'class_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>...</th>\n",
       "      <th>GL NA</th>\n",
       "      <th>Principal Component 1</th>\n",
       "      <th>Principal Component 2</th>\n",
       "      <th>Principal Component 3</th>\n",
       "      <th>Component 1</th>\n",
       "      <th>Component 2</th>\n",
       "      <th>Component 3</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.736724</td>\n",
       "      <td>0.757774</td>\n",
       "      <td>1.436346</td>\n",
       "      <td>0.088313</td>\n",
       "      <td>2.250189</td>\n",
       "      <td>0.656392</td>\n",
       "      <td>2.313644</td>\n",
       "      <td>-0.360044</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>-0.315629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.847444</td>\n",
       "      <td>-0.556655</td>\n",
       "      <td>-2.254345</td>\n",
       "      <td>5.141848</td>\n",
       "      <td>2.222537</td>\n",
       "      <td>7.899397</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>2.317108</td>\n",
       "      <td>1.503537</td>\n",
       "      <td>1.974562</td>\n",
       "      <td>0.140576</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>0.207984</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>1.482328</td>\n",
       "      <td>0.883370</td>\n",
       "      <td>-0.979664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.266880</td>\n",
       "      <td>4.949832</td>\n",
       "      <td>2.783413</td>\n",
       "      <td>3.707348</td>\n",
       "      <td>2.964474</td>\n",
       "      <td>3.666760</td>\n",
       "      <td>0.869463</td>\n",
       "      <td>0.130537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-0.626481</td>\n",
       "      <td>0.815893</td>\n",
       "      <td>-0.695633</td>\n",
       "      <td>-0.851478</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>-1.553574</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>-1.859163</td>\n",
       "      <td>0.236021</td>\n",
       "      <td>1.450303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.217254</td>\n",
       "      <td>-2.743147</td>\n",
       "      <td>-2.081434</td>\n",
       "      <td>3.562594</td>\n",
       "      <td>0.843929</td>\n",
       "      <td>8.515654</td>\n",
       "      <td>0.960368</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.426909</td>\n",
       "      <td>0.109778</td>\n",
       "      <td>1.963855</td>\n",
       "      <td>-1.724151</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>0.109355</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>0.180939</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>-0.254734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.513639</td>\n",
       "      <td>-0.866089</td>\n",
       "      <td>0.545431</td>\n",
       "      <td>2.214004</td>\n",
       "      <td>1.184474</td>\n",
       "      <td>4.919132</td>\n",
       "      <td>0.987938</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AB        AF        AH        AM        AR        AX        AY  \\\n",
       "325  0.736724  0.757774  1.436346  0.088313  2.250189  0.656392  2.313644   \n",
       "585  2.317108  1.503537  1.974562  0.140576 -0.488129  0.207984 -0.587024   \n",
       "274 -0.626481  0.815893 -0.695633 -0.851478 -0.488129 -1.553574 -0.587024   \n",
       "421  0.426909  0.109778  1.963855 -1.724151 -0.488129  0.109355 -0.587024   \n",
       "\n",
       "           AZ        BC       BD   ...  GL NA  Principal Component 1  \\\n",
       "325 -0.360044 -0.832691 -0.315629  ...    0.0               3.847444   \n",
       "585  1.482328  0.883370 -0.979664  ...    0.0               2.266880   \n",
       "274 -1.859163  0.236021  1.450303  ...    0.0               1.217254   \n",
       "421  0.180939 -0.832691 -0.254734  ...    0.0              -1.513639   \n",
       "\n",
       "     Principal Component 2  Principal Component 3  Component 1  Component 2  \\\n",
       "325              -0.556655              -2.254345     5.141848     2.222537   \n",
       "585               4.949832               2.783413     3.707348     2.964474   \n",
       "274              -2.743147              -2.081434     3.562594     0.843929   \n",
       "421              -0.866089               0.545431     2.214004     1.184474   \n",
       "\n",
       "     Component 3   class_0   class_1  prediction  \n",
       "325     7.899397  0.000031  0.999969           1  \n",
       "585     3.666760  0.869463  0.130537           0  \n",
       "274     8.515654  0.960368  0.039632           0  \n",
       "421     4.919132  0.987938  0.012062           0  \n",
       "\n",
       "[4 rows x 73 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n",
      "585\n",
      "274\n",
      "421\n"
     ]
    }
   ],
   "source": [
    "for i in x_val_check.index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['Class'] = y_train\n",
    "x_val['Class'] = y_val\n",
    "df_combinded = pd.concat([x_train, x_val], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>...</th>\n",
       "      <th>FL NA</th>\n",
       "      <th>FS NA</th>\n",
       "      <th>GL NA</th>\n",
       "      <th>Principal Component 1</th>\n",
       "      <th>Principal Component 2</th>\n",
       "      <th>Principal Component 3</th>\n",
       "      <th>Component 1</th>\n",
       "      <th>Component 2</th>\n",
       "      <th>Component 3</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.426909</td>\n",
       "      <td>0.109778</td>\n",
       "      <td>1.963855</td>\n",
       "      <td>-1.724151</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>0.109355</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>0.180939</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>-0.254734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.513639</td>\n",
       "      <td>-0.866089</td>\n",
       "      <td>0.545431</td>\n",
       "      <td>2.214004</td>\n",
       "      <td>1.184474</td>\n",
       "      <td>4.919132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>-0.846531</td>\n",
       "      <td>-0.551441</td>\n",
       "      <td>1.911839</td>\n",
       "      <td>-0.862598</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>-0.271585</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>1.248823</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>0.609656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.583374</td>\n",
       "      <td>0.966339</td>\n",
       "      <td>1.842023</td>\n",
       "      <td>2.405185</td>\n",
       "      <td>1.971125</td>\n",
       "      <td>3.437793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.559000</td>\n",
       "      <td>-0.264293</td>\n",
       "      <td>1.558152</td>\n",
       "      <td>0.967792</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>-0.519545</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>0.421436</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>0.437472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.865060</td>\n",
       "      <td>0.935584</td>\n",
       "      <td>1.160145</td>\n",
       "      <td>4.143613</td>\n",
       "      <td>2.535591</td>\n",
       "      <td>6.911425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1.399484</td>\n",
       "      <td>0.162439</td>\n",
       "      <td>1.800722</td>\n",
       "      <td>-0.232541</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>0.447366</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>0.635231</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>0.365590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.726604</td>\n",
       "      <td>-0.126038</td>\n",
       "      <td>1.591000</td>\n",
       "      <td>3.651320</td>\n",
       "      <td>2.202704</td>\n",
       "      <td>7.005035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1.454880</td>\n",
       "      <td>-0.608690</td>\n",
       "      <td>0.039722</td>\n",
       "      <td>0.416654</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>-0.036486</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>1.516080</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>-0.211885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.843944</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>-0.559997</td>\n",
       "      <td>3.881594</td>\n",
       "      <td>0.637803</td>\n",
       "      <td>6.990204</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>-0.509441</td>\n",
       "      <td>-0.693079</td>\n",
       "      <td>-0.695633</td>\n",
       "      <td>-0.630802</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>0.431927</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>2.469087</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>1.035300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.090524</td>\n",
       "      <td>-0.706221</td>\n",
       "      <td>-0.136729</td>\n",
       "      <td>4.243049</td>\n",
       "      <td>1.695152</td>\n",
       "      <td>7.176996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-0.143425</td>\n",
       "      <td>-0.532653</td>\n",
       "      <td>1.967888</td>\n",
       "      <td>-1.174909</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>-0.959962</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>1.135677</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>0.029441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.949306</td>\n",
       "      <td>2.852614</td>\n",
       "      <td>1.725369</td>\n",
       "      <td>2.661206</td>\n",
       "      <td>1.861383</td>\n",
       "      <td>3.485517</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>-1.236629</td>\n",
       "      <td>-0.445794</td>\n",
       "      <td>0.412359</td>\n",
       "      <td>-0.725944</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>0.773773</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>0.111190</td>\n",
       "      <td>-0.832691</td>\n",
       "      <td>-0.573033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.237120</td>\n",
       "      <td>-0.045810</td>\n",
       "      <td>1.185023</td>\n",
       "      <td>2.040678</td>\n",
       "      <td>1.646154</td>\n",
       "      <td>3.427182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.072898</td>\n",
       "      <td>0.256762</td>\n",
       "      <td>0.470460</td>\n",
       "      <td>-0.241490</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>0.180994</td>\n",
       "      <td>1.896648</td>\n",
       "      <td>1.520216</td>\n",
       "      <td>0.793699</td>\n",
       "      <td>0.587724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.323546</td>\n",
       "      <td>1.467533</td>\n",
       "      <td>0.508024</td>\n",
       "      <td>3.147249</td>\n",
       "      <td>1.712457</td>\n",
       "      <td>3.975017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-1.646437</td>\n",
       "      <td>-0.136181</td>\n",
       "      <td>1.301892</td>\n",
       "      <td>-0.957759</td>\n",
       "      <td>-0.488129</td>\n",
       "      <td>-0.353815</td>\n",
       "      <td>-0.587024</td>\n",
       "      <td>0.369346</td>\n",
       "      <td>0.439057</td>\n",
       "      <td>0.171040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.719825</td>\n",
       "      <td>-1.704433</td>\n",
       "      <td>3.012390</td>\n",
       "      <td>1.500517</td>\n",
       "      <td>1.368528</td>\n",
       "      <td>3.906818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AB        AF        AH        AM        AR        AX        AY  \\\n",
       "421  0.426909  0.109778  1.963855 -1.724151 -0.488129  0.109355 -0.587024   \n",
       "416 -0.846531 -0.551441  1.911839 -0.862598 -0.488129 -0.271585 -0.587024   \n",
       "12   0.559000 -0.264293  1.558152  0.967792 -0.488129 -0.519545 -0.587024   \n",
       "313  1.399484  0.162439  1.800722 -0.232541 -0.488129  0.447366 -0.587024   \n",
       "90  -1.454880 -0.608690  0.039722  0.416654 -0.488129 -0.036486 -0.587024   \n",
       "459 -0.509441 -0.693079 -0.695633 -0.630802 -0.488129  0.431927 -0.587024   \n",
       "566 -0.143425 -0.532653  1.967888 -1.174909  0.017692 -0.959962 -0.587024   \n",
       "254 -1.236629 -0.445794  0.412359 -0.725944 -0.488129  0.773773 -0.587024   \n",
       "164 -0.072898  0.256762  0.470460 -0.241490 -0.488129  0.180994  1.896648   \n",
       "440 -1.646437 -0.136181  1.301892 -0.957759 -0.488129 -0.353815 -0.587024   \n",
       "\n",
       "           AZ        BC       BD   ...  FL NA     FS NA  GL NA  \\\n",
       "421  0.180939 -0.832691 -0.254734  ...    0.0 -0.063823    0.0   \n",
       "416  1.248823 -0.832691  0.609656  ...    0.0 -0.063823    0.0   \n",
       "12   0.421436 -0.832691  0.437472  ...    0.0 -0.063823    0.0   \n",
       "313  0.635231 -0.832691  0.365590  ...    0.0 -0.063823    0.0   \n",
       "90   1.516080 -0.832691 -0.211885  ...    0.0 -0.063823    0.0   \n",
       "459  2.469087 -0.832691  1.035300  ...    0.0 -0.063823    0.0   \n",
       "566  1.135677 -0.832691  0.029441  ...    0.0 -0.063823    0.0   \n",
       "254  0.111190 -0.832691 -0.573033  ...    0.0 -0.063823    0.0   \n",
       "164  1.520216  0.793699  0.587724  ...    0.0 -0.063823    0.0   \n",
       "440  0.369346  0.439057  0.171040  ...    0.0 -0.063823    0.0   \n",
       "\n",
       "     Principal Component 1  Principal Component 2  Principal Component 3  \\\n",
       "421              -1.513639              -0.866089               0.545431   \n",
       "416              -2.583374               0.966339               1.842023   \n",
       "12                1.865060               0.935584               1.160145   \n",
       "313               0.726604              -0.126038               1.591000   \n",
       "90               -0.843944               0.022703              -0.559997   \n",
       "459               1.090524              -0.706221              -0.136729   \n",
       "566              -1.949306               2.852614               1.725369   \n",
       "254              -3.237120              -0.045810               1.185023   \n",
       "164              -1.323546               1.467533               0.508024   \n",
       "440              -3.719825              -1.704433               3.012390   \n",
       "\n",
       "     Component 1  Component 2  Component 3  Class  \n",
       "421     2.214004     1.184474     4.919132      1  \n",
       "416     2.405185     1.971125     3.437793      0  \n",
       "12      4.143613     2.535591     6.911425      0  \n",
       "313     3.651320     2.202704     7.005035      1  \n",
       "90      3.881594     0.637803     6.990204      0  \n",
       "459     4.243049     1.695152     7.176996      0  \n",
       "566     2.661206     1.861383     3.485517      0  \n",
       "254     2.040678     1.646154     3.427182      0  \n",
       "164     3.147249     1.712457     3.975017      0  \n",
       "440     1.500517     1.368528     3.906818      0  \n",
       "\n",
       "[10 rows x 71 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = 421\n",
    "n_neighbors = 10\n",
    "find_nearest_neighbors(df_combinded, row, n_neighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apple_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
